{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import __version__\n",
    "import numpy as np\n",
    "from keras.models import model_from_json, Model, Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from tempfile import TemporaryFile\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clothing_classifier():\n",
    "    weights_path = 'inceptionv3_clothing_classifier.h5'\n",
    "    json_path = 'incep_filter_clothing_classifier.json'\n",
    "\n",
    "    json_file = open(json_path, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weights_path)\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n",
    "\n",
    "def get_clothing_vector_model():\n",
    "    loaded_model = get_clothing_classifier()\n",
    "    loaded_model.layers.pop() # Get rid of the classification layer\n",
    "    last = loaded_model.layers[-1].output\n",
    "    model = Model(loaded_model.input, last)\n",
    "    return model\n",
    "\n",
    "def image_preprocess(img_path):\n",
    "    image = load_img(img_path, target_size=(299, 299))\n",
    "    image = img_to_array(image)\n",
    "\n",
    "    # our input image is now represented as a NumPy array of shape\n",
    "    # (inputShape[0], inputShape[1], 3) however we need to expand the\n",
    "    # dimension by making the shape (1, inputShape[0], inputShape[1], 3)\n",
    "    # so we can pass it through thenetwork\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # pre-process the image using the appropriate function based on the\n",
    "    # model that has been loaded (i.e., mean subtraction, scaling, etc.)\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "def get_classier_prediction(clothing_classifier, img_path):\n",
    "    clothes_labels = ['dresses', 'jackets', 'jeans', 'shorts', 'skirts', \n",
    "                      'sweaters', 'sweatshirts', 'womens-outerwear',\n",
    "                      'womens-pants', 'womens-tops']\n",
    "    img = image_preprocess(img_path)\n",
    "    preds = clothing_classifier.predict(img)[0]\n",
    "    preds_labels = list(zip(clothes_labels, preds))\n",
    "    preds_labels.sort(key=lambda p: p[1], reverse=True)\n",
    "    return preds_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, None, None, 3) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, None, None, 32 864         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, None, None, 32 96          conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, None, None, 32 0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, None, None, 32 9216        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, None, None, 32 96          conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, None, None, 32 0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, None, None, 64 18432       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, None, None, 64 192         conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, None, None, 64 0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, None, None, 64 0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, None, None, 80 5120        max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, None, None, 80 240         conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, None, None, 80 0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, None, None, 19 138240      activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, None, None, 19 576         conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, None, None, 19 0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, None, None, 19 0           activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, None, None, 64 12288       max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, None, None, 64 192         conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, None, None, 64 0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, None, None, 48 9216        max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, None, None, 96 55296       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, None, None, 48 144         conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, None, None, 96 288         conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, None, None, 48 0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, None, None, 96 0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePool (None, None, None, 19 0           max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, None, None, 64 12288       max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, None, None, 64 76800       activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, None, None, 96 82944       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, None, None, 32 6144        average_pooling2d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, None, None, 64 192         conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, None, None, 64 192         conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, None, None, 96 288         conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, None, None, 32 96          conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, None, None, 64 0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, None, None, 64 0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, None, None, 96 0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, None, None, 32 0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)             (None, None, None, 25 0           activation_6[0][0]               \n",
      "                                                                   activation_8[0][0]               \n",
      "                                                                   activation_11[0][0]              \n",
      "                                                                   activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)               (None, None, None, 64 16384       mixed0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, None, None, 64 192         conv2d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, None, None, 64 0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, None, None, 48 12288       mixed0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)               (None, None, None, 96 55296       activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, None, None, 48 144         conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, None, None, 96 288         conv2d_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, None, None, 48 0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, None, None, 96 0           batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePool (None, None, None, 25 0           mixed0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, None, None, 64 16384       mixed0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, None, None, 64 76800       activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)               (None, None, None, 96 82944       activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)               (None, None, None, 64 16384       average_pooling2d_2[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, None, None, 64 192         conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, None, None, 64 192         conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNor (None, None, None, 96 288         conv2d_18[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNor (None, None, None, 64 192         conv2d_19[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, None, None, 64 0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, None, None, 64 0           batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, None, None, 96 0           batch_normalization_18[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, None, None, 64 0           batch_normalization_19[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)             (None, None, None, 28 0           activation_13[0][0]              \n",
      "                                                                   activation_15[0][0]              \n",
      "                                                                   activation_18[0][0]              \n",
      "                                                                   activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)               (None, None, None, 64 18432       mixed1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNor (None, None, None, 64 192         conv2d_23[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, None, None, 64 0           batch_normalization_23[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)               (None, None, None, 48 13824       mixed1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)               (None, None, None, 96 55296       activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNor (None, None, None, 48 144         conv2d_21[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNor (None, None, None, 96 288         conv2d_24[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, None, None, 48 0           batch_normalization_21[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, None, None, 96 0           batch_normalization_24[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePool (None, None, None, 28 0           mixed1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)               (None, None, None, 64 18432       mixed1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)               (None, None, None, 64 76800       activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)               (None, None, None, 96 82944       activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)               (None, None, None, 64 18432       average_pooling2d_3[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNor (None, None, None, 64 192         conv2d_20[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNor (None, None, None, 64 192         conv2d_22[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNor (None, None, None, 96 288         conv2d_25[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNor (None, None, None, 64 192         conv2d_26[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, None, None, 64 0           batch_normalization_20[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, None, None, 64 0           batch_normalization_22[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, None, None, 96 0           batch_normalization_25[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, None, None, 64 0           batch_normalization_26[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)             (None, None, None, 28 0           activation_20[0][0]              \n",
      "                                                                   activation_22[0][0]              \n",
      "                                                                   activation_25[0][0]              \n",
      "                                                                   activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)               (None, None, None, 64 18432       mixed2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNor (None, None, None, 64 192         conv2d_28[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_28 (Activation)       (None, None, None, 64 0           batch_normalization_28[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)               (None, None, None, 96 55296       activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNor (None, None, None, 96 288         conv2d_29[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_29 (Activation)       (None, None, None, 96 0           batch_normalization_29[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)               (None, None, None, 38 995328      mixed2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)               (None, None, None, 96 82944       activation_29[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNor (None, None, None, 38 1152        conv2d_27[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNor (None, None, None, 96 288         conv2d_30[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, None, None, 38 0           batch_normalization_27[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, None, None, 96 0           batch_normalization_30[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, None, None, 28 0           mixed2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)             (None, None, None, 76 0           activation_27[0][0]              \n",
      "                                                                   activation_30[0][0]              \n",
      "                                                                   max_pooling2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)               (None, None, None, 12 98304       mixed3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNor (None, None, None, 12 384         conv2d_35[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, None, None, 12 0           batch_normalization_35[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)               (None, None, None, 12 114688      activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNor (None, None, None, 12 384         conv2d_36[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, None, None, 12 0           batch_normalization_36[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)               (None, None, None, 12 98304       mixed3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)               (None, None, None, 12 114688      activation_36[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNor (None, None, None, 12 384         conv2d_32[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNor (None, None, None, 12 384         conv2d_37[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, None, None, 12 0           batch_normalization_32[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_37 (Activation)       (None, None, None, 12 0           batch_normalization_37[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)               (None, None, None, 12 114688      activation_32[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)               (None, None, None, 12 114688      activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNor (None, None, None, 12 384         conv2d_33[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNor (None, None, None, 12 384         conv2d_38[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_33 (Activation)       (None, None, None, 12 0           batch_normalization_33[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_38 (Activation)       (None, None, None, 12 0           batch_normalization_38[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePool (None, None, None, 76 0           mixed3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)               (None, None, None, 19 147456      mixed3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)               (None, None, None, 19 172032      activation_33[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)               (None, None, None, 19 172032      activation_38[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)               (None, None, None, 19 147456      average_pooling2d_4[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNor (None, None, None, 19 576         conv2d_31[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNor (None, None, None, 19 576         conv2d_34[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNor (None, None, None, 19 576         conv2d_39[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNor (None, None, None, 19 576         conv2d_40[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, None, None, 19 0           batch_normalization_31[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, None, None, 19 0           batch_normalization_34[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_39 (Activation)       (None, None, None, 19 0           batch_normalization_39[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_40 (Activation)       (None, None, None, 19 0           batch_normalization_40[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)             (None, None, None, 76 0           activation_31[0][0]              \n",
      "                                                                   activation_34[0][0]              \n",
      "                                                                   activation_39[0][0]              \n",
      "                                                                   activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)               (None, None, None, 16 122880      mixed4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNor (None, None, None, 16 480         conv2d_45[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_45 (Activation)       (None, None, None, 16 0           batch_normalization_45[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)               (None, None, None, 16 179200      activation_45[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNor (None, None, None, 16 480         conv2d_46[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_46 (Activation)       (None, None, None, 16 0           batch_normalization_46[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)               (None, None, None, 16 122880      mixed4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)               (None, None, None, 16 179200      activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNor (None, None, None, 16 480         conv2d_42[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNor (None, None, None, 16 480         conv2d_47[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_42 (Activation)       (None, None, None, 16 0           batch_normalization_42[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_47 (Activation)       (None, None, None, 16 0           batch_normalization_47[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)               (None, None, None, 16 179200      activation_42[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)               (None, None, None, 16 179200      activation_47[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNor (None, None, None, 16 480         conv2d_43[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNor (None, None, None, 16 480         conv2d_48[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_43 (Activation)       (None, None, None, 16 0           batch_normalization_43[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_48 (Activation)       (None, None, None, 16 0           batch_normalization_48[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePool (None, None, None, 76 0           mixed4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)               (None, None, None, 19 147456      mixed4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)               (None, None, None, 19 215040      activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)               (None, None, None, 19 215040      activation_48[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)               (None, None, None, 19 147456      average_pooling2d_5[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNor (None, None, None, 19 576         conv2d_41[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNor (None, None, None, 19 576         conv2d_44[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNor (None, None, None, 19 576         conv2d_49[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNor (None, None, None, 19 576         conv2d_50[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_41 (Activation)       (None, None, None, 19 0           batch_normalization_41[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_44 (Activation)       (None, None, None, 19 0           batch_normalization_44[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_49 (Activation)       (None, None, None, 19 0           batch_normalization_49[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_50 (Activation)       (None, None, None, 19 0           batch_normalization_50[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)             (None, None, None, 76 0           activation_41[0][0]              \n",
      "                                                                   activation_44[0][0]              \n",
      "                                                                   activation_49[0][0]              \n",
      "                                                                   activation_50[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)               (None, None, None, 16 122880      mixed5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNor (None, None, None, 16 480         conv2d_55[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_55 (Activation)       (None, None, None, 16 0           batch_normalization_55[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)               (None, None, None, 16 179200      activation_55[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNor (None, None, None, 16 480         conv2d_56[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_56 (Activation)       (None, None, None, 16 0           batch_normalization_56[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)               (None, None, None, 16 122880      mixed5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)               (None, None, None, 16 179200      activation_56[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNor (None, None, None, 16 480         conv2d_52[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNor (None, None, None, 16 480         conv2d_57[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_52 (Activation)       (None, None, None, 16 0           batch_normalization_52[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_57 (Activation)       (None, None, None, 16 0           batch_normalization_57[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)               (None, None, None, 16 179200      activation_52[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)               (None, None, None, 16 179200      activation_57[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNor (None, None, None, 16 480         conv2d_53[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNor (None, None, None, 16 480         conv2d_58[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_53 (Activation)       (None, None, None, 16 0           batch_normalization_53[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_58 (Activation)       (None, None, None, 16 0           batch_normalization_58[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePool (None, None, None, 76 0           mixed5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)               (None, None, None, 19 147456      mixed5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)               (None, None, None, 19 215040      activation_53[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)               (None, None, None, 19 215040      activation_58[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)               (None, None, None, 19 147456      average_pooling2d_6[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNor (None, None, None, 19 576         conv2d_51[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNor (None, None, None, 19 576         conv2d_54[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNor (None, None, None, 19 576         conv2d_59[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNor (None, None, None, 19 576         conv2d_60[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_51 (Activation)       (None, None, None, 19 0           batch_normalization_51[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_54 (Activation)       (None, None, None, 19 0           batch_normalization_54[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_59 (Activation)       (None, None, None, 19 0           batch_normalization_59[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_60 (Activation)       (None, None, None, 19 0           batch_normalization_60[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)             (None, None, None, 76 0           activation_51[0][0]              \n",
      "                                                                   activation_54[0][0]              \n",
      "                                                                   activation_59[0][0]              \n",
      "                                                                   activation_60[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)               (None, None, None, 19 147456      mixed6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNor (None, None, None, 19 576         conv2d_65[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_65 (Activation)       (None, None, None, 19 0           batch_normalization_65[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)               (None, None, None, 19 258048      activation_65[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNor (None, None, None, 19 576         conv2d_66[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_66 (Activation)       (None, None, None, 19 0           batch_normalization_66[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)               (None, None, None, 19 147456      mixed6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)               (None, None, None, 19 258048      activation_66[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNor (None, None, None, 19 576         conv2d_62[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNor (None, None, None, 19 576         conv2d_67[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_62 (Activation)       (None, None, None, 19 0           batch_normalization_62[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_67 (Activation)       (None, None, None, 19 0           batch_normalization_67[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)               (None, None, None, 19 258048      activation_62[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)               (None, None, None, 19 258048      activation_67[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNor (None, None, None, 19 576         conv2d_63[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNor (None, None, None, 19 576         conv2d_68[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_63 (Activation)       (None, None, None, 19 0           batch_normalization_63[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_68 (Activation)       (None, None, None, 19 0           batch_normalization_68[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePool (None, None, None, 76 0           mixed6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)               (None, None, None, 19 147456      mixed6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)               (None, None, None, 19 258048      activation_63[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)               (None, None, None, 19 258048      activation_68[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)               (None, None, None, 19 147456      average_pooling2d_7[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNor (None, None, None, 19 576         conv2d_61[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNor (None, None, None, 19 576         conv2d_64[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNor (None, None, None, 19 576         conv2d_69[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNor (None, None, None, 19 576         conv2d_70[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_61 (Activation)       (None, None, None, 19 0           batch_normalization_61[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_64 (Activation)       (None, None, None, 19 0           batch_normalization_64[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_69 (Activation)       (None, None, None, 19 0           batch_normalization_69[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_70 (Activation)       (None, None, None, 19 0           batch_normalization_70[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)             (None, None, None, 76 0           activation_61[0][0]              \n",
      "                                                                   activation_64[0][0]              \n",
      "                                                                   activation_69[0][0]              \n",
      "                                                                   activation_70[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)               (None, None, None, 19 147456      mixed7[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNor (None, None, None, 19 576         conv2d_73[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_73 (Activation)       (None, None, None, 19 0           batch_normalization_73[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)               (None, None, None, 19 258048      activation_73[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNor (None, None, None, 19 576         conv2d_74[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_74 (Activation)       (None, None, None, 19 0           batch_normalization_74[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)               (None, None, None, 19 147456      mixed7[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)               (None, None, None, 19 258048      activation_74[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNor (None, None, None, 19 576         conv2d_71[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNor (None, None, None, 19 576         conv2d_75[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_71 (Activation)       (None, None, None, 19 0           batch_normalization_71[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_75 (Activation)       (None, None, None, 19 0           batch_normalization_75[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)               (None, None, None, 32 552960      activation_71[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)               (None, None, None, 19 331776      activation_75[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNor (None, None, None, 32 960         conv2d_72[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNor (None, None, None, 19 576         conv2d_76[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_72 (Activation)       (None, None, None, 32 0           batch_normalization_72[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_76 (Activation)       (None, None, None, 19 0           batch_normalization_76[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)   (None, None, None, 76 0           mixed7[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)             (None, None, None, 12 0           activation_72[0][0]              \n",
      "                                                                   activation_76[0][0]              \n",
      "                                                                   max_pooling2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)               (None, None, None, 44 573440      mixed8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNor (None, None, None, 44 1344        conv2d_81[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_81 (Activation)       (None, None, None, 44 0           batch_normalization_81[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)               (None, None, None, 38 491520      mixed8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)               (None, None, None, 38 1548288     activation_81[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNor (None, None, None, 38 1152        conv2d_78[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNor (None, None, None, 38 1152        conv2d_82[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_78 (Activation)       (None, None, None, 38 0           batch_normalization_78[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_82 (Activation)       (None, None, None, 38 0           batch_normalization_82[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)               (None, None, None, 38 442368      activation_78[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)               (None, None, None, 38 442368      activation_78[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)               (None, None, None, 38 442368      activation_82[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)               (None, None, None, 38 442368      activation_82[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePool (None, None, None, 12 0           mixed8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)               (None, None, None, 32 409600      mixed8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNor (None, None, None, 38 1152        conv2d_79[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNor (None, None, None, 38 1152        conv2d_80[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNor (None, None, None, 38 1152        conv2d_83[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNor (None, None, None, 38 1152        conv2d_84[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)               (None, None, None, 19 245760      average_pooling2d_8[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNor (None, None, None, 32 960         conv2d_77[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_79 (Activation)       (None, None, None, 38 0           batch_normalization_79[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_80 (Activation)       (None, None, None, 38 0           batch_normalization_80[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_83 (Activation)       (None, None, None, 38 0           batch_normalization_83[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_84 (Activation)       (None, None, None, 38 0           batch_normalization_84[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNor (None, None, None, 19 576         conv2d_85[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_77 (Activation)       (None, None, None, 32 0           batch_normalization_77[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)           (None, None, None, 76 0           activation_79[0][0]              \n",
      "                                                                   activation_80[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, None, None, 76 0           activation_83[0][0]              \n",
      "                                                                   activation_84[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_85 (Activation)       (None, None, None, 19 0           batch_normalization_85[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)             (None, None, None, 20 0           activation_77[0][0]              \n",
      "                                                                   mixed9_0[0][0]                   \n",
      "                                                                   concatenate_1[0][0]              \n",
      "                                                                   activation_85[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)               (None, None, None, 44 917504      mixed9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNor (None, None, None, 44 1344        conv2d_90[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_90 (Activation)       (None, None, None, 44 0           batch_normalization_90[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)               (None, None, None, 38 786432      mixed9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)               (None, None, None, 38 1548288     activation_90[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNor (None, None, None, 38 1152        conv2d_87[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNor (None, None, None, 38 1152        conv2d_91[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_87 (Activation)       (None, None, None, 38 0           batch_normalization_87[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_91 (Activation)       (None, None, None, 38 0           batch_normalization_91[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)               (None, None, None, 38 442368      activation_87[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)               (None, None, None, 38 442368      activation_87[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)               (None, None, None, 38 442368      activation_91[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)               (None, None, None, 38 442368      activation_91[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePool (None, None, None, 20 0           mixed9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)               (None, None, None, 32 655360      mixed9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNor (None, None, None, 38 1152        conv2d_88[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNor (None, None, None, 38 1152        conv2d_89[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNor (None, None, None, 38 1152        conv2d_92[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNor (None, None, None, 38 1152        conv2d_93[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)               (None, None, None, 19 393216      average_pooling2d_9[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNor (None, None, None, 32 960         conv2d_86[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_88 (Activation)       (None, None, None, 38 0           batch_normalization_88[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_89 (Activation)       (None, None, None, 38 0           batch_normalization_89[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_92 (Activation)       (None, None, None, 38 0           batch_normalization_92[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_93 (Activation)       (None, None, None, 38 0           batch_normalization_93[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNor (None, None, None, 19 576         conv2d_94[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_86 (Activation)       (None, None, None, 32 0           batch_normalization_86[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)           (None, None, None, 76 0           activation_88[0][0]              \n",
      "                                                                   activation_89[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, None, None, 76 0           activation_92[0][0]              \n",
      "                                                                   activation_93[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_94 (Activation)       (None, None, None, 19 0           batch_normalization_94[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)            (None, None, None, 20 0           activation_86[0][0]              \n",
      "                                                                   mixed9_1[0][0]                   \n",
      "                                                                   concatenate_2[0][0]              \n",
      "                                                                   activation_94[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glob (None, 2048)          0           mixed10[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1024)          2098176     global_average_pooling2d_1[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 10)            10250       dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 23,911,210\n",
      "Trainable params: 18,324,362\n",
      "Non-trainable params: 5,586,848\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clothing_classifier = get_clothing_classifier()\n",
    "clothing_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dresses', 0.99905783),\n",
       " ('skirts', 0.00062312855),\n",
       " ('womens-tops', 0.00027859688),\n",
       " ('womens-pants', 1.842705e-05),\n",
       " ('womens-outerwear', 1.5460646e-05),\n",
       " ('shorts', 3.0073475e-06),\n",
       " ('sweaters', 1.3901767e-06),\n",
       " ('jeans', 1.3824557e-06),\n",
       " ('jackets', 5.0633832e-07),\n",
       " ('sweatshirts', 3.2763697e-07)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeans_img_path = 'data-all/validation/jeans/482888849.jpg'\n",
    "dress_img_path = 'data-all/validation/dresses/535993294.jpg'\n",
    "jackets_img_path = 'data-all/validation/jackets/614521319.jpg'\n",
    "jackets_img_path2 = 'data-all/validation/jackets/614763686.jpg'\n",
    "dog_img_path = 'data-pets/train/dogs/dog.0.jpg'\n",
    "get_classier_prediction(clothing_classifier, dress_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.12710714,  0.        ,  0.66271973, ...,  0.2265493 ,\n",
       "         0.15804155,  1.66009367]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_vector_model = get_clothing_vector_model()\n",
    "img_jacket = image_preprocess(jackets_img_path)\n",
    "img_jacket2 = image_preprocess(jackets_img_path2)\n",
    "img_jeans = image_preprocess(jeans_img_path)\n",
    "img_dress = image_preprocess(dress_img_path)\n",
    "img_dog = image_preprocess(dog_img_path)\n",
    "\n",
    "jacket_feat = classifier_vector_model.predict(img_jacket)\n",
    "jacket_feat2 = classifier_vector_model.predict(img_jacket2)\n",
    "jeans_feat = classifier_vector_model.predict(img_jeans)\n",
    "dress_feat = classifier_vector_model.predict(img_dress)\n",
    "dog_feat = classifier_vector_model.predict(img_dog)\n",
    "\n",
    "jacket_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "2 jackets:  [[ 0.98380375]]\n",
      "jeans and dress:  [[ 0.96236557]]\n",
      "jeans and dog:  [[ 0.96911967]]\n"
     ]
    }
   ],
   "source": [
    "# compare cosine similarities\n",
    "print('2 jackets: ', cosine_similarity(jacket_feat, jacket_feat2))\n",
    "print('jeans and dress: ', cosine_similarity(jeans_feat, dress_feat))\n",
    "print('jeans and dog: ', cosine_similarity(jeans_feat, dog_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacket_feat.shape # (1, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Skipgrams Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_product_features(model, product_file, img_dir, output_file):\n",
    "    products = []\n",
    "    print('Reading file ', product_file)\n",
    "    with open(product_file, 'r') as f:\n",
    "        for i, line in enumerate(f.readlines()[:]):\n",
    "            if i % 50 == 0:\n",
    "                print('Processing product ', i)\n",
    "            try:\n",
    "                l = line.split('\\t')\n",
    "                product_id = l[0]\n",
    "                product_img_path = img_dir + product_id + '.jpg' \n",
    "                product_img = image_preprocess(product_img_path)\n",
    "                product_feat = model.predict(product_img)\n",
    "    \n",
    "                products.append([product_id] + product_feat[0].tolist())\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "    print('Saving products to ', output_file)\n",
    "    with open(output_file, 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        for p in products:\n",
    "            writer.writerow(p)\n",
    "            f.flush()\n",
    "    return products\n",
    "\n",
    "def get_product_permutation_inputs(outfit_permutations_file, product_to_features):\n",
    "    outfit_features = [] # each row = X, y, id\n",
    "    print('Reading file ', outfit_permutations_file)\n",
    "    with open(outfit_permutations_file, 'r') as f:\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            try:\n",
    "                l = line.split()\n",
    "                product1_id = l[2]\n",
    "                product2_id = l[5]\n",
    "                \n",
    "                X_products = [(product1_id, l[3])]\n",
    "                y_product = (product2_id, l[6])\n",
    "                \n",
    "                product1_feat = product_to_features[product1_id]\n",
    "                product2_feat = product_to_features[product2_id]\n",
    "                \n",
    "                # merged_feat = np.concatenate((product1_feat, product2_feat), axis=1)\n",
    "                # outfit_features.append((merged_feat, y_output, outfit_id))\n",
    "                outfit_features.append([product1_feat, product2_feat, \n",
    "                                        X_products, y_product])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    random.shuffle(outfit_features)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    X_products = []\n",
    "    y_products = []\n",
    "    for outfit in outfit_features:\n",
    "        X.append(outfit[0])\n",
    "        y.append(outfit[1])\n",
    "        X_products.append(outfit[2])\n",
    "        y_products.append(outfit[3])\n",
    "    return np.array(X), np.array(y), X_products, y_products\n",
    "\n",
    "def get_product_to_features(product_feats_file):\n",
    "    product_to_feats = {}\n",
    "    with open(product_feats_file, 'r') as tsvfile:\n",
    "        tsvreader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in tsvreader:\n",
    "            product_id = row[0]\n",
    "            feats = np.array([[float(n) for n in row[1:]]])\n",
    "            product_to_feats[product_id] = feats\n",
    "    return product_to_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file  data-outfits/outfit_products.tsv\n",
      "Processing product  0\n",
      "Processing product  50\n",
      "Processing product  100\n",
      "Processing product  150\n",
      "Processing product  200\n",
      "Processing product  250\n",
      "Processing product  300\n",
      "Processing product  350\n",
      "Processing product  400\n",
      "Processing product  450\n",
      "Processing product  500\n",
      "Processing product  550\n",
      "Processing product  600\n",
      "Processing product  650\n",
      "Processing product  700\n",
      "Processing product  750\n",
      "Processing product  800\n",
      "Processing product  850\n",
      "Processing product  900\n",
      "Processing product  950\n",
      "Processing product  1000\n",
      "Processing product  1050\n",
      "Processing product  1100\n",
      "Processing product  1150\n",
      "Processing product  1200\n",
      "Processing product  1250\n",
      "Processing product  1300\n",
      "Processing product  1350\n",
      "Processing product  1400\n",
      "Processing product  1450\n",
      "Processing product  1500\n",
      "Processing product  1550\n",
      "Processing product  1600\n",
      "Processing product  1650\n",
      "Processing product  1700\n",
      "Processing product  1750\n",
      "Processing product  1800\n",
      "Processing product  1850\n",
      "Processing product  1900\n",
      "Processing product  1950\n",
      "Processing product  2000\n",
      "Processing product  2050\n",
      "Processing product  2100\n",
      "Processing product  2150\n",
      "Processing product  2200\n",
      "Processing product  2250\n",
      "Processing product  2300\n",
      "Processing product  2350\n",
      "Processing product  2400\n",
      "Processing product  2450\n",
      "Processing product  2500\n",
      "Processing product  2550\n",
      "Processing product  2600\n",
      "Processing product  2650\n",
      "Processing product  2700\n",
      "Processing product  2750\n",
      "Processing product  2800\n",
      "Processing product  2850\n",
      "Processing product  2900\n",
      "Processing product  2950\n",
      "Processing product  3000\n",
      "Processing product  3050\n",
      "Processing product  3100\n",
      "Processing product  3150\n",
      "Processing product  3200\n",
      "Processing product  3250\n",
      "Processing product  3300\n",
      "Processing product  3350\n",
      "Processing product  3400\n",
      "Processing product  3450\n",
      "Processing product  3500\n",
      "Processing product  3550\n",
      "Processing product  3600\n",
      "Processing product  3650\n",
      "Processing product  3700\n",
      "Processing product  3750\n",
      "Processing product  3800\n",
      "Processing product  3850\n",
      "Processing product  3900\n",
      "Processing product  3950\n",
      "Processing product  4000\n",
      "Processing product  4050\n",
      "Processing product  4100\n",
      "Processing product  4150\n",
      "Processing product  4200\n",
      "Processing product  4250\n",
      "Processing product  4300\n",
      "Processing product  4350\n",
      "Processing product  4400\n",
      "Processing product  4450\n",
      "Processing product  4500\n",
      "Processing product  4550\n",
      "Processing product  4600\n",
      "Processing product  4650\n",
      "Processing product  4700\n",
      "Processing product  4750\n",
      "Processing product  4800\n",
      "Processing product  4850\n",
      "Processing product  4900\n",
      "Processing product  4950\n",
      "Processing product  5000\n",
      "Processing product  5050\n",
      "Processing product  5100\n",
      "Processing product  5150\n",
      "Processing product  5200\n",
      "Processing product  5250\n",
      "Processing product  5300\n",
      "Processing product  5350\n",
      "Processing product  5400\n",
      "Processing product  5450\n",
      "Processing product  5500\n",
      "Processing product  5550\n",
      "Processing product  5600\n",
      "Processing product  5650\n",
      "Processing product  5700\n",
      "Processing product  5750\n",
      "Processing product  5800\n",
      "Processing product  5850\n",
      "Processing product  5900\n",
      "Processing product  5950\n",
      "Processing product  6000\n",
      "Processing product  6050\n",
      "Processing product  6100\n",
      "Processing product  6150\n",
      "Processing product  6200\n",
      "Processing product  6250\n",
      "Processing product  6300\n",
      "Processing product  6350\n",
      "Processing product  6400\n",
      "Processing product  6450\n",
      "Processing product  6500\n",
      "Processing product  6550\n",
      "Processing product  6600\n",
      "Processing product  6650\n",
      "Processing product  6700\n",
      "Processing product  6750\n",
      "Processing product  6800\n",
      "Processing product  6850\n",
      "Processing product  6900\n",
      "Processing product  6950\n",
      "Processing product  7000\n",
      "Processing product  7050\n",
      "Processing product  7100\n",
      "Processing product  7150\n",
      "Processing product  7200\n",
      "Processing product  7250\n",
      "Processing product  7300\n",
      "Processing product  7350\n",
      "Processing product  7400\n",
      "Processing product  7450\n",
      "Processing product  7500\n",
      "Processing product  7550\n",
      "Processing product  7600\n",
      "Processing product  7650\n",
      "Processing product  7700\n",
      "Processing product  7750\n",
      "Processing product  7800\n",
      "Processing product  7850\n",
      "Saving products to  data-outfits/outfit_product_features.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "# Run one time to save product weights\n",
    "# save_product_features(classifier_vector_model, 'data-outfits/outfit_products.tsv', \n",
    "#                       'data-outfits/images/', 'data-outfits/outfit_product_features.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.87435913,  0.        ,  0.14283158, ...,  0.51789564,\n",
       "         0.30757272,  0.92094642]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_feats_file = 'data-outfits/outfit_product_features.tsv'\n",
    "outfit_permutations_file = 'data-outfits/outfit_permutations.tsv'\n",
    "prods_to_feats = get_product_to_features(product_feats_file)\n",
    "prods_to_feats['615752261']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file  data-outfits/outfit_permutations.tsv\n"
     ]
    }
   ],
   "source": [
    "(X, y, X_products, y_products) = get_product_permutation_inputs(outfit_permutations_file, prods_to_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12002, 1, 1024)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X).shape # (400, 1, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_skipgrams_clothing_model():\n",
    "    seq = Sequential()\n",
    "    # seq.add(Dense(128, input_shape=(1,2048), activation='relu', name='fc1'))\n",
    "    seq.add(Dense(1024, input_shape=(1,1024), activation='relu', name='fc1'))\n",
    "    seq.add(Dropout(0.2))\n",
    "    seq.add(Dense(256, activation='relu', name='fc2'))\n",
    "    seq.add(Dropout(0.1))\n",
    "    #seq.add(Flatten())\n",
    "    seq.add(Dense(1024, activation='softmax', name='fc_final'))\n",
    "    seq.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.RMSprop(lr=0.00001), metrics=[\"accuracy\"])\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "fc1 (Dense)                  (None, 1, 1024)           1049600   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 1, 256)            262400    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "fc_final (Dense)             (None, 1, 1024)           263168    \n",
      "=================================================================\n",
      "Total params: 1,575,168\n",
      "Trainable params: 1,575,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8401 samples, validate on 3601 samples\n",
      "Epoch 1/100\n",
      "8401/8401 [==============================] - 8s - loss: 5556.2298 - acc: 0.1347 - val_loss: 5342.5439 - val_acc: 0.4854\n",
      "Epoch 2/100\n",
      "8401/8401 [==============================] - 7s - loss: 5346.3055 - acc: 0.1835 - val_loss: 5316.4360 - val_acc: 0.7392\n",
      "Epoch 3/100\n",
      "8401/8401 [==============================] - 7s - loss: 5320.8339 - acc: 0.2564 - val_loss: 5308.4976 - val_acc: 0.7617\n",
      "Epoch 4/100\n",
      "8401/8401 [==============================] - 7s - loss: 5308.7592 - acc: 0.3112 - val_loss: 5302.5106 - val_acc: 0.8636\n",
      "Epoch 5/100\n",
      "8401/8401 [==============================] - 7s - loss: 5301.9604 - acc: 0.3559 - val_loss: 5299.9082 - val_acc: 0.8478\n",
      "Epoch 6/100\n",
      "8401/8401 [==============================] - 7s - loss: 5298.2526 - acc: 0.3836 - val_loss: 5298.4889 - val_acc: 0.9142\n",
      "Epoch 7/100\n",
      "8401/8401 [==============================] - 7s - loss: 5295.8824 - acc: 0.4254 - val_loss: 5298.0517 - val_acc: 0.8759\n",
      "Epoch 8/100\n",
      "8401/8401 [==============================] - 7s - loss: 5294.3832 - acc: 0.4542 - val_loss: 5297.2260 - val_acc: 0.9214\n",
      "Epoch 9/100\n",
      "8401/8401 [==============================] - 7s - loss: 5293.1625 - acc: 0.4847 - val_loss: 5298.5749 - val_acc: 0.9089\n",
      "Epoch 10/100\n",
      "8401/8401 [==============================] - 7s - loss: 5292.3752 - acc: 0.5039 - val_loss: 5297.1553 - val_acc: 0.9292\n",
      "Epoch 11/100\n",
      "8401/8401 [==============================] - 7s - loss: 5291.7138 - acc: 0.5235 - val_loss: 5297.0860 - val_acc: 0.9256\n",
      "Epoch 12/100\n",
      "8401/8401 [==============================] - 7s - loss: 5291.1613 - acc: 0.5404 - val_loss: 5296.3381 - val_acc: 0.9331\n",
      "Epoch 13/100\n",
      "8401/8401 [==============================] - 7s - loss: 5290.6056 - acc: 0.5609 - val_loss: 5296.3609 - val_acc: 0.9336\n",
      "Epoch 14/100\n",
      "8401/8401 [==============================] - 7s - loss: 5290.2525 - acc: 0.5717 - val_loss: 5297.6684 - val_acc: 0.9086\n",
      "Epoch 15/100\n",
      "8401/8401 [==============================] - 7s - loss: 5289.8578 - acc: 0.5833 - val_loss: 5296.6963 - val_acc: 0.9256\n",
      "Epoch 16/100\n",
      "8401/8401 [==============================] - 7s - loss: 5289.6129 - acc: 0.6098 - val_loss: 5296.0701 - val_acc: 0.9314\n",
      "Epoch 17/100\n",
      "8401/8401 [==============================] - 7s - loss: 5289.2957 - acc: 0.6147 - val_loss: 5296.3863 - val_acc: 0.9350\n",
      "Epoch 18/100\n",
      "8401/8401 [==============================] - 7s - loss: 5289.0660 - acc: 0.6361 - val_loss: 5297.1415 - val_acc: 0.9203\n",
      "Epoch 19/100\n",
      "8401/8401 [==============================] - 7s - loss: 5288.8070 - acc: 0.6411 - val_loss: 5296.0762 - val_acc: 0.9320\n",
      "Epoch 20/100\n",
      "8401/8401 [==============================] - 7s - loss: 5288.6202 - acc: 0.6558 - val_loss: 5295.6610 - val_acc: 0.9386\n",
      "Epoch 21/100\n",
      "8401/8401 [==============================] - 7s - loss: 5288.4657 - acc: 0.6680 - val_loss: 5295.6734 - val_acc: 0.9414\n",
      "Epoch 22/100\n",
      "8401/8401 [==============================] - 7s - loss: 5288.3141 - acc: 0.6812 - val_loss: 5296.5615 - val_acc: 0.9353\n",
      "Epoch 23/100\n",
      "8401/8401 [==============================] - 7s - loss: 5288.1578 - acc: 0.6841 - val_loss: 5295.9739 - val_acc: 0.9328\n",
      "Epoch 24/100\n",
      "8401/8401 [==============================] - 7s - loss: 5288.0100 - acc: 0.6872 - val_loss: 5296.1024 - val_acc: 0.9359\n",
      "Epoch 25/100\n",
      "8401/8401 [==============================] - 7s - loss: 5287.8852 - acc: 0.7043 - val_loss: 5295.4487 - val_acc: 0.9408\n",
      "Epoch 26/100\n",
      "8401/8401 [==============================] - 7s - loss: 5287.7736 - acc: 0.7100 - val_loss: 5295.5209 - val_acc: 0.9420\n",
      "Epoch 27/100\n",
      "8401/8401 [==============================] - 7s - loss: 5287.6704 - acc: 0.7365 - val_loss: 5295.9119 - val_acc: 0.9395\n",
      "Epoch 28/100\n",
      "8401/8401 [==============================] - 7s - loss: 5287.5643 - acc: 0.7311 - val_loss: 5295.3631 - val_acc: 0.9372\n",
      "Epoch 29/100\n",
      "8401/8401 [==============================] - 7s - loss: 5287.4743 - acc: 0.7343 - val_loss: 5295.5375 - val_acc: 0.9422\n",
      "Epoch 30/100\n",
      "8401/8401 [==============================] - 7s - loss: 5287.3304 - acc: 0.7430 - val_loss: 5296.9643 - val_acc: 0.9428\n",
      "Epoch 31/100\n",
      "8401/8401 [==============================] - 7s - loss: 5287.2026 - acc: 0.7621 - val_loss: 5295.6300 - val_acc: 0.9422\n",
      "Epoch 32/100\n",
      "8401/8401 [==============================] - 7s - loss: 5287.1566 - acc: 0.7662 - val_loss: 5295.7966 - val_acc: 0.9414\n",
      "Epoch 33/100\n",
      "8401/8401 [==============================] - 7s - loss: 5287.1377 - acc: 0.7669 - val_loss: 5295.1536 - val_acc: 0.9411\n",
      "Epoch 34/100\n",
      "8401/8401 [==============================] - 7s - loss: 5287.0003 - acc: 0.7841 - val_loss: 5295.1450 - val_acc: 0.9428\n",
      "Epoch 35/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.9165 - acc: 0.7816 - val_loss: 5296.4666 - val_acc: 0.9422\n",
      "Epoch 36/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.8412 - acc: 0.7976 - val_loss: 5296.1690 - val_acc: 0.9431\n",
      "Epoch 37/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.7544 - acc: 0.7987 - val_loss: 5295.2363 - val_acc: 0.9425\n",
      "Epoch 38/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.6969 - acc: 0.8076 - val_loss: 5295.0571 - val_acc: 0.9422\n",
      "Epoch 39/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.6229 - acc: 0.8092 - val_loss: 5294.9567 - val_acc: 0.9422\n",
      "Epoch 40/100\n",
      "8401/8401 [==============================] - 8s - loss: 5286.5658 - acc: 0.8076 - val_loss: 5294.9540 - val_acc: 0.9420\n",
      "Epoch 41/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.4910 - acc: 0.8214 - val_loss: 5296.4118 - val_acc: 0.9411\n",
      "Epoch 42/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.4967 - acc: 0.8238 - val_loss: 5295.3816 - val_acc: 0.9431\n",
      "Epoch 43/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.4092 - acc: 0.8236 - val_loss: 5295.3411 - val_acc: 0.9428\n",
      "Epoch 44/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.3447 - acc: 0.8312 - val_loss: 5295.5387 - val_acc: 0.9431\n",
      "Epoch 45/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.3216 - acc: 0.8393 - val_loss: 5294.8935 - val_acc: 0.9428\n",
      "Epoch 46/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.2094 - acc: 0.8412 - val_loss: 5295.6484 - val_acc: 0.9431\n",
      "Epoch 47/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.1654 - acc: 0.8410 - val_loss: 5294.8640 - val_acc: 0.9425\n",
      "Epoch 48/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.1066 - acc: 0.8484 - val_loss: 5296.6283 - val_acc: 0.9428\n",
      "Epoch 49/100\n",
      "8401/8401 [==============================] - 7s - loss: 5286.0701 - acc: 0.8529 - val_loss: 5295.2746 - val_acc: 0.9428\n",
      "Epoch 50/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.9733 - acc: 0.8549 - val_loss: 5294.9073 - val_acc: 0.9431\n",
      "Epoch 51/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.9244 - acc: 0.8622 - val_loss: 5294.9096 - val_acc: 0.9431\n",
      "Epoch 52/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.9062 - acc: 0.8704 - val_loss: 5295.1997 - val_acc: 0.9431\n",
      "Epoch 53/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.8273 - acc: 0.8642 - val_loss: 5295.4636 - val_acc: 0.9428\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8401/8401 [==============================] - 7s - loss: 5285.7897 - acc: 0.8672 - val_loss: 5295.3579 - val_acc: 0.9431\n",
      "Epoch 55/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.7642 - acc: 0.8804 - val_loss: 5294.6908 - val_acc: 0.9428\n",
      "Epoch 56/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.7343 - acc: 0.8753 - val_loss: 5294.9574 - val_acc: 0.9431\n",
      "Epoch 57/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.6891 - acc: 0.8831 - val_loss: 5295.1310 - val_acc: 0.9428\n",
      "Epoch 58/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.6544 - acc: 0.8830 - val_loss: 5294.9266 - val_acc: 0.9431\n",
      "Epoch 59/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.6215 - acc: 0.8876 - val_loss: 5294.7821 - val_acc: 0.9431\n",
      "Epoch 60/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.5790 - acc: 0.8883 - val_loss: 5294.6967 - val_acc: 0.9431\n",
      "Epoch 61/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.5724 - acc: 0.8944 - val_loss: 5294.7944 - val_acc: 0.9431\n",
      "Epoch 62/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.4480 - acc: 0.8960 - val_loss: 5294.6891 - val_acc: 0.9428\n",
      "Epoch 63/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.4528 - acc: 0.8928 - val_loss: 5294.7300 - val_acc: 0.9431\n",
      "Epoch 64/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.4079 - acc: 0.9011 - val_loss: 5294.7313 - val_acc: 0.9431\n",
      "Epoch 65/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.3139 - acc: 0.8993 - val_loss: 5294.6958 - val_acc: 0.9431\n",
      "Epoch 66/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.3043 - acc: 0.8987 - val_loss: 5294.7567 - val_acc: 0.9431\n",
      "Epoch 67/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.2854 - acc: 0.9095 - val_loss: 5295.5882 - val_acc: 0.9431\n",
      "Epoch 68/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.2108 - acc: 0.9058 - val_loss: 5294.8109 - val_acc: 0.9431\n",
      "Epoch 69/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.2152 - acc: 0.9097 - val_loss: 5294.7959 - val_acc: 0.9431\n",
      "Epoch 70/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.1694 - acc: 0.9056 - val_loss: 5295.0485 - val_acc: 0.9431\n",
      "Epoch 71/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.0948 - acc: 0.9114 - val_loss: 5294.7489 - val_acc: 0.9431\n",
      "Epoch 72/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.1144 - acc: 0.9138 - val_loss: 5294.5323 - val_acc: 0.9431\n",
      "Epoch 73/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.0674 - acc: 0.9127 - val_loss: 5294.8579 - val_acc: 0.9431\n",
      "Epoch 74/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.0575 - acc: 0.9174 - val_loss: 5294.5698 - val_acc: 0.9428\n",
      "Epoch 75/100\n",
      "8401/8401 [==============================] - 7s - loss: 5285.0064 - acc: 0.9186 - val_loss: 5295.5202 - val_acc: 0.9431\n",
      "Epoch 76/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.9723 - acc: 0.9181 - val_loss: 5294.7949 - val_acc: 0.9431\n",
      "Epoch 77/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.8717 - acc: 0.9202 - val_loss: 5294.4764 - val_acc: 0.9431\n",
      "Epoch 78/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.8931 - acc: 0.9197 - val_loss: 5294.7915 - val_acc: 0.9431\n",
      "Epoch 79/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.7796 - acc: 0.9238 - val_loss: 5295.3511 - val_acc: 0.9431\n",
      "Epoch 80/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.8218 - acc: 0.9207 - val_loss: 5294.5614 - val_acc: 0.9431\n",
      "Epoch 81/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.8203 - acc: 0.9276 - val_loss: 5295.0211 - val_acc: 0.9431\n",
      "Epoch 82/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.7944 - acc: 0.9243 - val_loss: 5294.4422 - val_acc: 0.9431\n",
      "Epoch 83/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.6928 - acc: 0.9266 - val_loss: 5294.7129 - val_acc: 0.9431\n",
      "Epoch 84/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.6670 - acc: 0.9285 - val_loss: 5294.4546 - val_acc: 0.9431\n",
      "Epoch 85/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.5875 - acc: 0.9267 - val_loss: 5294.5313 - val_acc: 0.9431\n",
      "Epoch 86/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.6448 - acc: 0.9274 - val_loss: 5294.6582 - val_acc: 0.9431\n",
      "Epoch 87/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.5476 - acc: 0.9277 - val_loss: 5294.6019 - val_acc: 0.9431\n",
      "Epoch 88/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.5848 - acc: 0.9307 - val_loss: 5294.9553 - val_acc: 0.9431\n",
      "Epoch 89/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.5203 - acc: 0.9302 - val_loss: 5294.7323 - val_acc: 0.9431\n",
      "Epoch 90/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.4795 - acc: 0.9307 - val_loss: 5294.8977 - val_acc: 0.9431\n",
      "Epoch 91/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.4558 - acc: 0.9295 - val_loss: 5294.8591 - val_acc: 0.9431\n",
      "Epoch 92/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.4426 - acc: 0.9322 - val_loss: 5294.4527 - val_acc: 0.9431\n",
      "Epoch 93/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.3787 - acc: 0.9327 - val_loss: 5294.4389 - val_acc: 0.9431\n",
      "Epoch 94/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.3598 - acc: 0.9318 - val_loss: 5294.5508 - val_acc: 0.9431\n",
      "Epoch 95/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.3427 - acc: 0.9333 - val_loss: 5294.4736 - val_acc: 0.9431\n",
      "Epoch 96/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.2667 - acc: 0.9341 - val_loss: 5294.4075 - val_acc: 0.9431\n",
      "Epoch 97/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.2486 - acc: 0.9351 - val_loss: 5294.4055 - val_acc: 0.9431\n",
      "Epoch 98/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.2432 - acc: 0.9338 - val_loss: 5294.4961 - val_acc: 0.9431\n",
      "Epoch 99/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.1911 - acc: 0.9345 - val_loss: 5294.2941 - val_acc: 0.9431\n",
      "Epoch 100/100\n",
      "8401/8401 [==============================] - 7s - loss: 5284.1752 - acc: 0.9348 - val_loss: 5294.3124 - val_acc: 0.9431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0699323ef0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clothing_skipgrams_model = make_skipgrams_clothing_model()\n",
    "clothing_skipgrams_model.summary()\n",
    "clothing_skipgrams_model.fit(X, y, validation_split=0.3, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([('534952171', ['jeans'])],\n",
       "  array([[  3.43282474e-03,   7.06519074e-07,   6.68733031e-04, ...,\n",
       "            3.50462709e-04,   2.63515685e-04,   8.52939906e-04]], dtype=float32)),\n",
       " ([('651257129', ['womens-tops'])],\n",
       "  array([[  3.19540524e-03,   1.06691246e-07,   4.43253375e-04, ...,\n",
       "            8.89367890e-04,   4.44327947e-04,   1.06019492e-03]], dtype=float32)),\n",
       " ([('649091558', ['womens-tops'])],\n",
       "  array([[  3.42820631e-03,   7.48775975e-09,   3.12320713e-04, ...,\n",
       "            9.37269884e-04,   3.72050970e-04,   1.05632527e-03]], dtype=float32))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clothing_predictions = list(zip(X_products, clothing_skipgrams_model.predict(X)))\n",
    "clothing_predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Input: ('534952171', ['jeans'])\n",
      "open -a Preview 534952171.jpg\n",
      "open -a Preview 655404693.jpg 607817031.jpg 538017987.jpg 614591849.jpg 625727413.jpg\n",
      "1 Input: ('651257129', ['womens-tops'])\n",
      "open -a Preview 651257129.jpg\n",
      "open -a Preview 628749781.jpg 536963805.jpg 503439151.jpg 477420618.jpg 511435611.jpg\n",
      "2 Input: ('649091558', ['womens-tops'])\n",
      "open -a Preview 649091558.jpg\n",
      "open -a Preview 503439151.jpg 634504599.jpg 628749781.jpg 655411082.jpg 639098704.jpg\n",
      "3 Input: ('532520135', ['womens-pants'])\n",
      "open -a Preview 532520135.jpg\n",
      "open -a Preview 655404693.jpg 607817031.jpg 538017987.jpg 505902236.jpg 614591849.jpg\n",
      "4 Input: ('603742660', ['womens-tops'])\n",
      "open -a Preview 603742660.jpg\n",
      "open -a Preview 628749781.jpg 503439151.jpg 606147530.jpg 634504599.jpg 541576780.jpg\n",
      "5 Input: ('495320947', ['womens-tops'])\n",
      "open -a Preview 495320947.jpg\n",
      "open -a Preview 628749781.jpg 503439151.jpg 634504599.jpg 639098704.jpg 606147530.jpg\n",
      "6 Input: ('271667299', ['womens-tops'])\n",
      "open -a Preview 271667299.jpg\n",
      "open -a Preview 628749781.jpg 503439151.jpg 536963805.jpg 634504599.jpg 642156755.jpg\n",
      "7 Input: ('525769960', ['shorts'])\n",
      "open -a Preview 525769960.jpg\n",
      "open -a Preview 655404693.jpg 607817031.jpg 644706812.jpg 638162902.jpg 511172811.jpg\n",
      "8 Input: ('619978090', ['jeans'])\n",
      "open -a Preview 619978090.jpg\n",
      "open -a Preview 655404693.jpg 607817031.jpg 511172811.jpg 644706812.jpg 638162902.jpg\n",
      "9 Input: ('630460395', ['womens-tops'])\n",
      "open -a Preview 630460395.jpg\n",
      "open -a Preview 503439151.jpg 628749781.jpg 634504599.jpg 655411082.jpg 639098704.jpg\n",
      "10 Input: ('534952297', ['jeans'])\n",
      "open -a Preview 534952297.jpg\n",
      "open -a Preview 655404693.jpg 607817031.jpg 538017987.jpg 614591849.jpg 511172811.jpg\n",
      "11 Input: ('544045896', ['sweaters'])\n",
      "open -a Preview 544045896.jpg\n",
      "open -a Preview 628749781.jpg 606147530.jpg 503439151.jpg 632096995.jpg 646054812.jpg\n",
      "12 Input: ('637612549', ['jeans'])\n",
      "open -a Preview 637612549.jpg\n",
      "open -a Preview 655404693.jpg 607817031.jpg 538017987.jpg 614591849.jpg 511172811.jpg\n",
      "13 Input: ('502987397', ['jeans'])\n",
      "open -a Preview 502987397.jpg\n",
      "open -a Preview 511172811.jpg 655404693.jpg 641079640.jpg 652979320.jpg 607817031.jpg\n",
      "14 Input: ('512296254', ['shorts'])\n",
      "open -a Preview 512296254.jpg\n",
      "open -a Preview 655590034.jpg 638557917.jpg 644391815.jpg 622230394.jpg 652979320.jpg\n",
      "15 Input: ('520682006', ['womens-tops'])\n",
      "open -a Preview 520682006.jpg\n",
      "open -a Preview 622623686.jpg 662575012.jpg 627921613.jpg 536963805.jpg 474091291.jpg\n",
      "16 Input: ('635953132', ['womens-tops'])\n",
      "open -a Preview 635953132.jpg\n",
      "open -a Preview 628749781.jpg 503439151.jpg 606147530.jpg 634504599.jpg 625555164.jpg\n",
      "17 Input: ('536961691', ['jeans'])\n",
      "open -a Preview 536961691.jpg\n",
      "open -a Preview 655404693.jpg 607817031.jpg 538017987.jpg 614591849.jpg 505902236.jpg\n",
      "18 Input: ('652262056', ['womens-tops'])\n",
      "open -a Preview 652262056.jpg\n",
      "open -a Preview 622623686.jpg 662575012.jpg 536963805.jpg 511435611.jpg 474091291.jpg\n",
      "19 Input: ('623101252', ['womens-tops'])\n",
      "open -a Preview 623101252.jpg\n",
      "open -a Preview 628749781.jpg 536963805.jpg 634504599.jpg 645958708.jpg 511435611.jpg\n"
     ]
    }
   ],
   "source": [
    "def get_closest_product_ids(input_vector):\n",
    "    prod_cosine_sims = []\n",
    "    for prod, prod_vector in prods_to_feats.items():\n",
    "        sim = cosine_similarity(input_vector, prod_vector)\n",
    "        prod_cosine_sims.append((prod, sim))\n",
    "    return sorted(prod_cosine_sims, key=lambda p: p[1], reverse=True)[:5]   \n",
    "        \n",
    "for i, pred in enumerate(clothing_predictions[:20]):\n",
    "    X_products, prediction_vector = pred \n",
    "    print('{} Input: {}'.format(i, X_products[0])) # Only one product for each X for now\n",
    "    print('open -a Preview {}'.format(X_products[0][0] + '.jpg'))\n",
    "    predicted_outputs = get_closest_product_ids(prediction_vector)\n",
    "    predicted_images = [p[0] + '.jpg' for p in predicted_outputs]\n",
    "    print('open -a Preview ' + ' '.join(predicted_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2, 3]])\n",
    "b = np.array([[4,5, 6]])\n",
    "ab = np.concatenate((a, b), axis=1)\n",
    "ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.np_utils.to_categorical(np.array([1, 0, 0, 0]), num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([[0]])\n",
    "np.concatenate((c, a), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.array([1, 2, 3, 4])\n",
    "np.array([0]) + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
